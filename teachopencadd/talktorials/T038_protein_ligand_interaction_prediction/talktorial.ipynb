{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T038 · Protein Ligand Interaction Prediction\n",
    "\n",
    "**Note:** This talktorial is a part of TeachOpenCADD, a platform that aims to teach domain-specific skills and to provide pipeline templates as starting points for research projects.\n",
    "\n",
    "Authors:\n",
    "\n",
    "- Roman Joeres, 2022, [Chair for Drug Bioinformatics, UdS and HIPS](https://www.helmholtz-hips.de/de/forschung/teams/team/wirkstoffbioinformatik/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aim of this talktorial\n",
    "\n",
    "The goal of this talktorial is to introduce the reader to the field of protein ligand interaction prediction using graph neural networks (GNNs). GNNs are especially useful for representing structural data such as proteins and chemical molecules (ligands) to a deep learning model. In this talktorial, I will show how to train a deep learning model to predict interactions between proteins and ligands."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contents in *Theory*\n",
    "\n",
    "* Relevance of Protein Ligand Interaction Prediction\n",
    "* Workflow\n",
    "* Biological background - Proteins as Graphs\n",
    "* Technical background\n",
    "  * Graph Isomorphism Networks\n",
    "  * Binary Cross Entropy Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contents in *Practical*\n",
    "\n",
    "* Compute graph representations\n",
    "  * Ligands to graphs\n",
    "  * Proteins to graphs\n",
    "* Data Storages\n",
    "  * Data Points\n",
    "  * Data Set\n",
    "  * Data Module\n",
    "* Network\n",
    "  * GNN Encoder\n",
    "  * Full Model\n",
    "* Training routine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "* Theoretical background\n",
    "    * Graph Neural Networks:\n",
    "      Kipf, Welling: \"Semi-Supervised Classification with Graph Convolutional Networks\", [<i>arXiv</i> (2017)](https://arxiv.org/abs/1609.02907)\n",
    "      Bronstein, et al.: \"Geometric deep learning: going beyond Euclidean data\", [<i>IEEE Signal Processing Magazine</i> (2017), <b>4</b>, 18-42](https://doi.org/10.1109/MSP.2017.2693418)\n",
    "    * GNN based Protein Ligand Interaction Prediction:\n",
    "      Öztürk, et al.: \"DeepDTA: Deep drug-target binding affinity prediction\", [<i>Bioinformatics</i> (2018), <b>34</b>, i821-i829](https://doi.org/10.1093/bioinformatics/bty593)\n",
    "      Nguyen, et. al.: \"GraphDTA: Predicting drug target binding affinity with graph neural networks\", [<i>Bioinformatics</i> (2021), <b>37</b>, 1140-1147](https://doi.org/10.1093/bioinformatics/btaa921)\n",
    "    * Graph Isomorphism Network:\n",
    "      Xu, et al.: \"How powerful are graph neural networks?\", [<i>arXiv</i> (2018)](https://arxiv.org/abs/1810.00826)\n",
    "\n",
    "* Practical Background\n",
    "    * [PyTorch](https://pytorch.org/)\n",
    "    * [PyTorch Geometric](https://pytorch-geometric.readthedocs.io/en/latest/)\n",
    "    * [RDKit](http://rdkit.org/): Greg Landrum, *RDKit Documentation*, [PDF](https://www.rdkit.org/UGM/2012/Landrum_RDKit_UGM.Fingerprints.Final.pptx.pdf), Release on 2019.09.1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theory\n",
    "\n",
    "This talktorial combines several topics, you have seen in other talktorials. Here, I will describe the general idea of how to predict interactions between proteins and ligands. If some technique used in the workflow is already presented somewhere else, I'll link to this otherwise, I'll explain new things below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relevance of Protein Ligand Interaction Prediction\n",
    "Protein-ligand interactions are of interest in research for many reasons as can be seen in [__Talktorial T016__](https://github.com/volkamerlab/teachopencadd/blob/master/teachopencadd/talktorials/T016_protein_ligand_interactions/talktorial.ipynb). One of the most important fields where being able to predict whether a certain pair of protein and ligand interact or not is the field of drug discovery. Classically, in drug development one want to find a new drug given a protein target. For example in the CoVid-19 pandemic, most pharmacy companies tried to find drugs to bind to the spike protein. In order TBC."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Workflow\n",
    "\n",
    "Our input is a dataset comprising a set of proteins and a set of ligands and a table with binding information for every pair of proteins and ligands. Speaking in terms of supervised learning (as in [__Talktorial T022__](https://github.com/volkamerlab/teachopencadd/blob/master/teachopencadd/talktorials/T022_ligand_based_screening_neural_networks/talktorial.ipynb)), our dataset we use for training, validation, and testing is the table of interaction data. One component of our network architecture is a simple Feed-forward Neural Network (FNN) as presented in [__Talktorial T022__](https://github.com/volkamerlab/teachopencadd/blob/master/teachopencadd/talktorials/T022_ligand_based_screening_neural_networks/talktorial.ipynb). The other two components are graph neural networks (GNNs) to extract features from the proteins and ligands in each pair of the dataset. As discussed in [__Talktorial T034__](https://github.com/volkamerlab/teachopencadd/blob/master/teachopencadd/talktorials/T034_link/talktorial.ipynb) GNNs are used to compute a representation of graph structured data that holds information about the structure. These representations are concatenated into one vector which serves as input for the final FNN.\n",
    "\n",
    "![Basic structure](./images/basic_structure.png)\n",
    "\n",
    "*Figure 1:*\n",
    "Visualization of the workflow of the model in this notebook. The shown exemplary structures are taken from from the PDB entry with ID 4O75."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Biological background - Proteins as Graphs\n",
    "\n",
    "Here, I will focus on the conversion of proteins into graphs as the conversion of SMILES to graphs is explained in [__Talktorial T034__](https://github.com/volkamerlab/teachopencadd/blob/master/teachopencadd/talktorials/T022_ligand_based_screening_neural_networks/talktorial.ipynb).\n",
    "\n",
    "There are usually two ways to represent proteins in science. Either by their sequence of amino acids or as a PDB structure as introduced in [__Talktorial T008__](https://github.com/volkamerlab/teachopencadd/blob/master/teachopencadd/talktorials/T008_query_pdb/talktorial.ipynb). As amino acid sequences do not contain structural information, in protein ligand interaction prediction we use PDB files of proteins as input for our structure-based models. In the graph representation of a protein, every node of the graph represents an amino acid from the protein. Edges between nodes in the graph are drawn if the two represented amino acids are within a certain distance. This is the equivalent of an interaction between the two amino acids in teh protein. To compute the distance of two amino acids, we look at the coordinates of the $C_\\alpha$ atoms of the amino acids in the PDB file. If the distance between two $C_\\alpha$ atoms is below a certain distance threshold, we consider the amino acids to interact and insert an edge in the graph representation of the protein. This can be seen in Figure 2.\n",
    "\n",
    "![Prot2Graph](./images/prot_graph_creation.png)\n",
    "\n",
    "*Figure 2:*\n",
    "Visualization of the process and idea of protein representation as graphs. For this example, we consider only the $C_\\alpha$ atoms of the cysteins to be withing a distance threshold of 7 Angstrom. As both cysteins are spatially close, their sulfates generate a disulfate bridge and stabilizes the protein three-dimensions structure which is the type of interaction we want to have in the graph representations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Technical background\n",
    "\n",
    "In this section, I will focus on the computer science aspects of the proposed solution. Mainly, I'll discuss the concrete GNN architecture to use and which node features I use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Graph Isomorphism Networks\n",
    "\n",
    "There is a whole zoo of GNN architectures proposed to solve many problems. If you want to get an overview over the most popular architectures, you can have a look at the [list of convolutional layers implemented in PyTorch-Geometric](https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html#convolutional-layers). In this talktorial, we will use the GINConv layers as backbone of out GNNs as they have been proven to be powerful in embedding molecular data yet remaining easy to understand in their functionality. The formula to compute an embedding of a node based on the neighbors is\n",
    "$$\\mathbf{x}^{\\prime}_i = h_{\\mathbf{\\Theta}} \\left( (1 + \\epsilon) \\cdot \\mathbf{x}_i + \\sum_{j \\in \\mathcal{N}(i)} \\mathbf{x}_j \\right)$$\n",
    "where $\\mathcal{N}(i)$ is the set of neighbors of node $i$, $\\epsilon$ is just a constant, and $h_{\\mathbf{\\Theta}}$ is a neural network as presented in [__Talktorial T022__](https://github.com/volkamerlab/teachopencadd/blob/master/teachopencadd/talktorials/T022_ligand_based_screening_neural_networks/talktorial.ipynb). The idea is to aggregate all neighbor embeddings together with the own current embedding and put this into a neural network to extract information on the nodes and it's neighborhood.\n",
    "\n",
    "As can be seen, GINConv layers do not use edge information in their computation. So, the only thing we need to extract from out proteins and ligands when turning into graphs are features for the edges. In this talktorial, we will use a very simple featurization and every node just contains categorical information on the amino acid type or atom type is represents. Information on onehot encodings of categorical data is covered in [__Talktorial T021__](https://github.com/volkamerlab/teachopencadd/blob/master/teachopencadd/talktorials/T022_one_hot_encoding/talktorial.ipynb).\n",
    "\n",
    "The final element to finalize our GNN module is the pooling function, that is used to compute the graph embedding based on the node embeddings in the final layer. For simplicity reasons (and because it's surprisingly powerful) we use mean pooling. That means, we just take the mean vector over all node embeddings in the final GINConv layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Binary Cross Entropy Loss\n",
    "\n",
    "[__Talktorial T022__](https://github.com/volkamerlab/teachopencadd/blob/master/teachopencadd/talktorials/T022_ligand_based_screening_neural_networks/talktorial.ipynb) introduces two loss functions, namely MSE and MAE. Both are suitable to train regression models but not appropriate for classification. For classification there is wide range of loss functions of which we will use the [Binary Cross Entropy Loss](https://pytorch.org/docs/stable/generated/torch.nn.BCELoss.html).\n",
    "\n",
    "The formula to compute the loss is\n",
    "$$-\\left[ y\\cdot\\log x+(1-y)\\cdot\\log (1-x)\\right],$$\n",
    "where $x$ is the models output for one sample and $y$ is the label of that sample.\n",
    "\n",
    "The idea is that exactly one term of $y$ and $1-y$ equals $1$ to the formula reduces to $\\log x$ for a positive sample and $\\log (1-x)$ for a negative example. By this setting, the BCE formula ensures that you want to push the predicted values $x$ towards 0 in negative samples ($y=0$) and towards $1$ in positive cases ($y=1$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this practical section, we will discuss every step in implementing the above presented solution to protein ligand interaction prediction. We will start with all the imports needed and some path definitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "from rdkit import Chem\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor, nn\n",
    "from torch.optim import Adam\n",
    "from torch_geometric.nn import global_mean_pool, GINConv\n",
    "from torch_geometric.data import Data, InMemoryDataset\n",
    "from torch_geometric.loader import DataLoader\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "HERE = Path(\"./\")\n",
    "DATA = HERE / \"data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute graph representations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ligands to graphs\n",
    "\n",
    "First, we're going to implement the conversion of ligands into graphs. For the following explaination, I assume the ligand has $N$ atoms. To encode a graph, we have to compute a matrix of the node features (a $N\\times F$-matrix where $F$ is the number of features per node) and a matrix of the edges given by pairs of the participating node ids.\n",
    "\n",
    "Due to some PyTorch Geometric related implementation details, the edge-matrix has to have the format $2\\times N$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For every atom type we consider, map the symbol to a numerical value for onehot encoding.\n",
    "atoms_to_num = dict((atom, i) for i, atom in enumerate([\n",
    "    \"C\", \"N\", \"O\", \"F\", \"P\", \"S\", \"Cl\", \"Br\", \"I\"\n",
    "]))\n",
    "\n",
    "\n",
    "def atom_to_onehot(atom):\n",
    "    \"\"\"\n",
    "    Return the onehot encoding for an atom given as it's index from the atoms_to_num dict.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    atom: str\n",
    "        Atomic symbol of the atom to represent\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    torch.Tensor\n",
    "        A onehot tensor encoding the atoms features.\n",
    "    \"\"\"\n",
    "    # initialize a 0-vector ...\n",
    "    one_hot = torch.zeros(len(atoms_to_num) + 1, dtype=torch.float)\n",
    "    # ... and set the according field to one, ...\n",
    "    if atom in atoms_to_num:\n",
    "        one_hot[atoms_to_num[atom]] = 1.0\n",
    "    # ... the last field is used to represent atom types that do not have their own field in the onehot vector\n",
    "    else:\n",
    "        one_hot[len(atoms_to_num)] = 1.0\n",
    "    return one_hot\n",
    "\n",
    "\n",
    "def smiles_to_graph(smiles):\n",
    "    \"\"\"\n",
    "    Convert a molecule given as SDF file into a graph.\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    smiles: str\n",
    "        Path to the file storing the structural information of the ligand\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Tuple[torch.Tensor, torch.Tensor]\n",
    "        A pair of node features and edges in the PyTorch Geometric format\n",
    "    \"\"\"\n",
    "    # read in the molecule from an SDF file\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    atoms, bonds = [], []\n",
    "    # check if the molecule is valid\n",
    "    if mol is None:\n",
    "        print(smiles)\n",
    "        return None, None\n",
    "\n",
    "    # iterate over all atom, compute the feature vector and store them in a torch.Tensor object\n",
    "    for atom in mol.GetAtoms():\n",
    "        atoms.append(atom_to_onehot(atom.GetSymbol()))\n",
    "    atoms = torch.stack(atoms)\n",
    "\n",
    "    # iterate over all bonds in the molecule and store them in the PyTorch Geometric specific format in a torch.Tensor,\n",
    "    for bond in mol.GetBonds():\n",
    "        bonds.append((bond.GetBeginAtomIdx(), bond.GetEndAtomIdx()))\n",
    "        bonds.append((bond.GetEndAtomIdx(), bond.GetBeginAtomIdx()))\n",
    "    bonds = torch.tensor(bonds, dtype=torch.long).T\n",
    "\n",
    "    return atoms, bonds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Proteins to graphs\n",
    "\n",
    "Similar to how we converted ligands to graphs, we convert proteins into graphs. The output will be the same, a pair of node features and edges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a mapping from amino acids to numbers for onehot encoding\n",
    "aa_to_num = dict((aa, i) for i, aa in enumerate([\n",
    "    \"ALA\", \"ARG\", \"ASN\", \"ASP\", \"CYS\", \"GLU\", \"GLN\", \"GLY\", \"HIS\", \"ILE\", \"LEU\", \"LYS\", \"MET\",  \"PHE\", \"PRO\",  \"SER\", \"THR\", \"TRP\", \"TYR\", \"VAL\", \"UNK\",\n",
    "]))\n",
    "\n",
    "\n",
    "def aa_to_onehot(aa):\n",
    "    \"\"\"\n",
    "    Compute the onehot vector for an amino acid representing node.\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    aa: str\n",
    "        The three-letter code of the amino acid to be represented\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    torch.Tensor\n",
    "        A onehot tensor encoding the atoms features.\n",
    "    \"\"\"\n",
    "    one_hot = torch.zeros(len(aa_to_num), dtype=torch.float)\n",
    "    one_hot[aa_to_num[aa]] = 1.0\n",
    "    return one_hot\n",
    "\n",
    "\n",
    "def pdb_to_graph(pdb_file_path, max_dist=7.0):\n",
    "    \"\"\"\n",
    "    Extract a graph representation of a protein from the PDB file.\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    pdb_file_path: str\n",
    "        Filepath of the PDB file containing structural information on the protein\n",
    "    max_dist: float\n",
    "        Distance threshold to apply when computing edges between amino acids\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Tuple[torch.Tensor, torch.Tensor]\n",
    "        A pair of node features and edges in the PyTorch Geometric format\n",
    "    \"\"\"\n",
    "    # read in the PDB file by looking for the Calpha atoms and extract their amino acid and coordinates based on the positioning in the PDB file\n",
    "    residues = []\n",
    "    with open(pdb_file_path, \"r\") as protein:\n",
    "        for line in protein:\n",
    "            if line.startswith(\"ATOM\") and line[12:16].strip() == \"CA\":\n",
    "                residues.append((\n",
    "                    line[17:20].strip(),\n",
    "                    float(line[30:38].strip()),\n",
    "                    float(line[38:46].strip()),\n",
    "                    float(line[46:54].strip()),\n",
    "                ))\n",
    "    # Finally compute the node features based on the amino acids in the protein\n",
    "    node_feat = torch.stack([aa_to_onehot(res[0]) for res in residues])\n",
    "\n",
    "    # compute the edges of the protein by iterating over all pairs of amino acids and computing their distance\n",
    "    edges = []\n",
    "    for i in range(len(residues)):\n",
    "        res = residues[i]\n",
    "        for j in range(i + 1, len(residues)):\n",
    "            tmp = residues[j]\n",
    "            if math.dist(res[1:4], tmp[1:4]) <= max_dist:\n",
    "                edges.append((i, j))\n",
    "                edges.append((j, i))\n",
    "\n",
    "    # store the edges in the PyTorch Geometric format\n",
    "    edges = torch.tensor(edges, dtype=torch.long).T\n",
    "\n",
    "    return node_feat, edges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Storages\n",
    "\n",
    "Storing and representing data in PLI-prediction is a bit different from other neural networks. Therefore, we have to define our own classes to represent the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Points\n",
    "\n",
    "Usually the built-in Data class of PyTorch Geometric is used to represent only one graph, for our task, the data contains two graphs, therefore, we need to adapt the functionality to compute the number of nodes and edges for one data point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DTIDataPair(Data):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    @property\n",
    "    def num_nodes(self):\n",
    "        return self[\"lig_x\"].size(0) + self[\"prot_x\"].size(0)\n",
    "\n",
    "    @property\n",
    "    def num_edges(self):\n",
    "        return self[\"lig_edge_index\"].size(1) + self[\"prot_edge_index\"].size(1)\n",
    "\n",
    "    def __inc__(self, key, value, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        Method that is necessary to overwrite for successful batching of DTIDataPair object.\n",
    "        In case of interest, one can look at this explanation:\n",
    "        https://pytorch-geometric.readthedocs.io/en/latest/notes/batching.html\n",
    "\n",
    "        When multiple samples are send through a network at once, they are aggregated into batches.\n",
    "        In PyTorch Geometric this is done by copying all n graphs for one batch into one graph with\n",
    "        n connected components. Because of this, the node ids in the edge_index objects have to be\n",
    "        changed. As they have to be increased by a fix offset based on the number of nodes in the\n",
    "        batch so far, this method computes this offset in case the edge_indices of either the\n",
    "        proteins or ligands.\n",
    "\n",
    "        Arguments\n",
    "        ---------\n",
    "        key: str\n",
    "            String name of the field of this class to increment while batching\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            A one-element tensor describing how to modify the values when batching.\n",
    "        \"\"\"\n",
    "        if not key.endswith(\"edge_index\"):\n",
    "            return super().__inc__(key, value, *args, **kwargs)\n",
    "        lenedg = len(\"edge_index\")\n",
    "        prefix = key[:-lenedg]\n",
    "        return self[prefix + \"x\"].size(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Set\n",
    "\n",
    "This is where the real data magic happens. In the dataset, we read in the data and process them into the graphical representation we want to have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DTIDataset(InMemoryDataset):\n",
    "    def __init__(self, folder_name, file_index):\n",
    "        self.folder_name = folder_name\n",
    "        super().__init__(root=folder_name)\n",
    "        self.data, self.slices = torch.load(self.processed_paths[file_index])\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        \"\"\"\n",
    "        Just store the names of the files where the training split, validation split, and test split are stored.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        List[str]\n",
    "            A list of filenames where the preprocessed data is stored to not recompute the preprocessing every time.\n",
    "        \"\"\"\n",
    "        return [\"train.pt\", \"val.pt\", \"test.pt\"]\n",
    "\n",
    "    def process(self):\n",
    "        \"\"\"\n",
    "        This function is called internally in the preprocessing routine of PyTorch Geometric and defined how the dataset of PDB files, ligands, and an interaction table is converted into a dataset of graphs, ready for deep learning.\n",
    "        \"\"\"\n",
    "        # compute all ligand graphs and store them as a dictionary with their names as key and the graphs as values\n",
    "        ligand_graphs = dict()\n",
    "        with open(os.path.join(self.folder_name, \"tables\", \"ligands.tsv\"), \"r\") as data:\n",
    "            for line in data.readlines()[1:]:\n",
    "                chembl_id, smiles = line.strip().split(\"\\t\")[:2]\n",
    "                ligand_graphs[chembl_id] = smiles_to_graph(smiles)\n",
    "\n",
    "        # compute all protein graphs and store them as a dictionary with their names as key and the graphs as values\n",
    "        protein_graphs = dict([\n",
    "            (filename[:-4], pdb_to_graph(os.path.join(os.path.join(self.folder_name, \"proteins\", filename)))) for filename in os.listdir(os.path.join(self.folder_name, \"proteins\"))\n",
    "        ])\n",
    "\n",
    "        with open(os.path.join(self.folder_name, \"tables\", \"inter.tsv\")) as inter:\n",
    "            data_list = []\n",
    "            for line in inter.readlines()[1:]:\n",
    "                # read a line with one interaction sample. Extract ligand and protein ID and get their graphs from the dictionaries above\n",
    "                protein, ligand, y = line.strip().split('\\t')\n",
    "                lig_node_feat, lig_edge_index = ligand_graphs[ligand]\n",
    "                prot_node_feat, prot_edge_index = protein_graphs[protein]\n",
    "\n",
    "                # if either ligand or protein are invalid graphs, skip this sample ...\n",
    "                if lig_node_feat is None or prot_node_feat is None:\n",
    "                    print(line.strip())\n",
    "                    continue\n",
    "\n",
    "                # ... otherwise, create a datapoint using the class from above\n",
    "                data_list.append(DTIDataPair(\n",
    "                    lig_x=lig_node_feat,\n",
    "                    lig_edge_index=lig_edge_index,\n",
    "                    prot_x=prot_node_feat,\n",
    "                    prot_edge_index=prot_edge_index,\n",
    "                    y=torch.tensor(float(y), dtype=torch.float),\n",
    "                ))\n",
    "\n",
    "            # shuffle the data, and compute how many samples go into which split\n",
    "            random.shuffle(data_list)\n",
    "            train_frac = int(len(data_list) * 0.7)\n",
    "            test_frac = int(len(data_list) * 0.1)\n",
    "\n",
    "            # then split the data and store them for later reuse without running the preprocessing pipeline\n",
    "            train_data, train_slices = self.collate(data_list[:train_frac])\n",
    "            torch.save((train_data, train_slices), self.processed_paths[0])\n",
    "            val_data, val_slices = self.collate(data_list[train_frac:-test_frac])\n",
    "            torch.save((val_data, val_slices), self.processed_paths[1])\n",
    "            test_data, test_slices = self.collate(data_list[-test_frac:])\n",
    "            torch.save((test_data, test_slices), self.processed_paths[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Module\n",
    "\n",
    "This is just a handy class holding all three splits of a dataset and providing data loaders for training, validation, and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DTIDataModule:\n",
    "    def __init__(self, folder_name):\n",
    "        self.train = DTIDataset(folder_name, 0)\n",
    "        self.val = DTIDataset(folder_name, 1)\n",
    "        self.test = DTIDataset(folder_name, 2)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        \"\"\"\n",
    "        Create and return a dataloader for the training dataset.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch_geometric.loaders.DataLoader\n",
    "            Dataloader on the training dataset\n",
    "        \"\"\"\n",
    "        return DataLoader(self.train, batch_size=64, shuffle=True, follow_batch=[\"prot_x\", \"lig_x\"])\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        \"\"\"\n",
    "        Create and return a dataloader for the validation dataset.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch_geometric.loaders.DataLoader\n",
    "            Dataloader on the validation dataset\n",
    "        \"\"\"\n",
    "        return DataLoader(self.val, batch_size=64, shuffle=True, follow_batch=[\"prot_x\", \"lig_x\"])\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        \"\"\"\n",
    "        Create and return a dataloader for the test dataset.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch_geometric.loaders.DataLoader\n",
    "            Dataloader on the test dataset\n",
    "        \"\"\"\n",
    "        return DataLoader(self.test, batch_size=64, shuffle=True, follow_batch=[\"prot_x\", \"lig_x\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network\n",
    "\n",
    "Here, we will implement the networks as defined in the theory section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GNN Encoder\n",
    "\n",
    "First, the GNN encoder which we will use for both, embedding proteins and embedding ligands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=128, output_dim=64, num_layers=3):\n",
    "        \"\"\"\n",
    "        Encoder to embed structural data using a stack of GINConv layers.\n",
    "\n",
    "        Arguments\n",
    "        ---------\n",
    "        input_dim: int\n",
    "            Size of the feature vector of the data\n",
    "        hidden_dim: int\n",
    "            Number of hidden neurons to use when computing the embeddings\n",
    "        output_dim: int\n",
    "            Size of the output vector of the final graph embedding after a final mean pooling\n",
    "        num_layers: int\n",
    "            Number of layers to use when computing embedding. This includes input and output layers, so values below 3 are meaningless.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.layers = [\n",
    "            # define the input layer\n",
    "            GINConv(\n",
    "                nn.Sequential(\n",
    "                    nn.Linear(input_dim, hidden_dim),\n",
    "                    nn.PReLU(),\n",
    "                    nn.Linear(hidden_dim, hidden_dim),\n",
    "                    nn.BatchNorm1d(hidden_dim),\n",
    "                )\n",
    "            )\n",
    "        ] + [\n",
    "            # define a number of hidden layers\n",
    "            GINConv(\n",
    "                nn.Sequential(\n",
    "                    nn.Linear(hidden_dim, hidden_dim),\n",
    "                    nn.PReLU(),\n",
    "                    nn.Linear(hidden_dim, hidden_dim),\n",
    "                    nn.BatchNorm1d(hidden_dim),\n",
    "                )\n",
    "            )\n",
    "            for _ in range(num_layers - 2)\n",
    "        ] + [\n",
    "            # define the output layer\n",
    "            GINConv(\n",
    "                nn.Sequential(\n",
    "                    nn.Linear(hidden_dim, hidden_dim),\n",
    "                    nn.PReLU(),\n",
    "                    nn.Linear(hidden_dim, output_dim),\n",
    "                    nn.BatchNorm1d(output_dim),\n",
    "                )\n",
    "            )\n",
    "        ]\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        \"\"\"\n",
    "        Forward a batch of samples through this network to compute the forward pass.\n",
    "\n",
    "        Arguments\n",
    "        ---------\n",
    "        x: torch.Tensor\n",
    "            feature matrices of the graphs forwarded through the network\n",
    "        edge_index: torch.Tensor\n",
    "            edge indices of the graphs forwarded through the network\n",
    "        batch: torch.Tensor\n",
    "            Some internally used information, not relevant for the topic of this talktorial\n",
    "        \"\"\"\n",
    "        for layer in self.layers:\n",
    "            x = layer(x=x, edge_index=edge_index)\n",
    "        pool = global_mean_pool(x, batch)\n",
    "        return F.normalize(pool, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Full Model\n",
    "\n",
    "Define the full model according to the workflow proposed in the theory section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DTINetwork(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # create encoders for both, proteins and ligands\n",
    "        self.prot_encoder = Encoder(21)\n",
    "        self.lig_encoder = Encoder(10)\n",
    "\n",
    "        # define a simple FNN to compute the final prediction (to bind or not to bind)\n",
    "        self.combine = torch.nn.Sequential(\n",
    "            torch.nn.Linear(128, 256),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(0.2),\n",
    "\n",
    "            torch.nn.Linear(256, 64),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(0.2),\n",
    "\n",
    "            torch.nn.Linear(64, 1),\n",
    "            torch.nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, data):\n",
    "        \"\"\"\n",
    "        Define the standard forward process of this network.\n",
    "\n",
    "        Arguments\n",
    "        ---------\n",
    "        data: DTIDataPairBatch\n",
    "            A batch of DTIDataPair samples to be predicted to train on them\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Prediction values for all pairs in the input batch\n",
    "        \"\"\"\n",
    "        # compute the protein embeddings using the protein embedder on the protein data of the batch\n",
    "        prot_embed = self.prot_encoder(\n",
    "            x=data.prot_x,\n",
    "            edge_index=data.prot_edge_index,\n",
    "            batch=data.prot_x_batch,\n",
    "        )\n",
    "\n",
    "        # compute the ligand embeddings using the ligand embedder on the ligand data of the batch\n",
    "        lig_embed = self.lig_encoder(\n",
    "            x=data.lig_x,\n",
    "            edge_index=data.lig_edge_index,\n",
    "            batch=data.lig_x_batch,\n",
    "        )\n",
    "\n",
    "        # concatenate both embeddings and return the output of the FNN\n",
    "        combined = torch.cat((prot_embed, lig_embed), dim=1)\n",
    "        return self.combine(combined)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Routine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(num_epochs=10):\n",
    "    \"\"\"\n",
    "    Implementation of the actual training routine.\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    num_epochs: int\n",
    "        Number of epochs to train the model\n",
    "    \"\"\"\n",
    "    # load the data, model, and define the loss function\n",
    "    dataset = DTIDataModule(DATA / \"resources\")\n",
    "    model = DTINetwork()\n",
    "    loss_fn = torch.nn.BCELoss()\n",
    "    optimizer = Adam(model.parameters(), lr=0.0001)\n",
    "    train_acc, train_loss, val_acc, val_loss = [], [], [], []\n",
    "\n",
    "    # train for num_epochs\n",
    "    for e in range(num_epochs):\n",
    "        print(f\"Epoch {e + 1}/{num_epochs}\")\n",
    "        epoch_train_acc, epoch_train_loss, epoch_val_acc, epoch_val_loss = [], [], [], []\n",
    "\n",
    "        # perform the actual training\n",
    "        train_loader = dataset.train_dataloader()\n",
    "        for b, data in enumerate(train_loader):\n",
    "            # compute the models predictions and the loss\n",
    "            pred = model.forward(data).squeeze()\n",
    "            loss = loss_fn(pred, data.y.squeeze())\n",
    "\n",
    "            # perform one step of backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # report some statistics on the training batch\n",
    "            pred = pred > 0.5\n",
    "            epoch_train_acc.append(sum(pred == data.y) / len(pred))\n",
    "            epoch_train_loss.append(loss.item())\n",
    "            print(f\"\\rTraining step {(b + 1)}/{len(train_loader)}: Loss: {epoch_train_loss[-1]:.5f}\\tAcc: {epoch_train_acc[-1]:.5f}\", end=\"\")\n",
    "        \n",
    "        train_acc.append(sum(epoch_train_acc) / len(epoch_train_acc))\n",
    "        train_loss.append(sum(epoch_train_loss) / len(epoch_train_loss))\n",
    "        print(f\"\\rTraining end epoch {e + 1}:   Loss: {train_loss[-1]:.5f}\\tAcc: {train_acc[-1]:.5f}\")\n",
    "        torch.save(model.state_dict(), DATA / f\"model_{e + 1}.pth\")\n",
    "\n",
    "        # perform validation of the last training epoch\n",
    "        val_loader = dataset.val_dataloader()\n",
    "        for b, data in enumerate(val_loader):\n",
    "            # compute the models predictions and the loss\n",
    "            pred = model.forward(data).squeeze()\n",
    "            loss = loss_fn(pred, data.y.squeeze())\n",
    "\n",
    "            # report some statistics on the validation batch\n",
    "            pred = pred > 0.5\n",
    "            epoch_val_acc.append(sum(pred == data.y) / len(pred))\n",
    "            epoch_val_loss.append(loss.item())\n",
    "            print(f\"\\rValidation step {(b + 1)}/{len(val_loader)}: Loss: {epoch_val_loss[-1]:.5f}\\tAcc: {epoch_val_acc[-1]:.5f}\", end=\"\")\n",
    "        \n",
    "        val_acc.append(sum(epoch_val_acc) / len(epoch_val_acc))\n",
    "        val_loss.append(sum(epoch_val_loss) / len(epoch_val_loss))\n",
    "        print(f\"\\rValidation end epoch {e + 1}: Loss: {val_loss[-1]:.5f}\\tAcc: {val_acc[-1]:.5f}\")\n",
    "\n",
    "    # test the final model\n",
    "    print()\n",
    "    test_loss, test_acc = [], []\n",
    "    val_loader = dataset.test_dataloader()\n",
    "    for b, data in enumerate(val_loader):\n",
    "        # compute the models predictions and the loss\n",
    "        pred = model.forward(data).squeeze()\n",
    "        loss = loss_fn(pred, data.y.squeeze())\n",
    "\n",
    "        # report some statistics on the validation batch\n",
    "        pred = pred > 0.5\n",
    "        test_acc.append(sum(pred == data.y) / len(pred))\n",
    "        test_loss.append(loss.item())\n",
    "        print(f\"\\rTesting Loss: {test_loss[-1]:.5f}\\tAcc: {test_acc[-1]:.5f}\", end=\"\")\n",
    "    print(f\"\\rTesting: Loss: {(sum(test_loss) / len(test_loss)):.5f}\\tAcc: {(sum(test_acc) / len(test_acc)):.5f}\")\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 2, figsize=(8, 4), sharey=True)\n",
    "    \n",
    "    ax[0].plot(train_loss, c=\"b\", label=\"Train\")\n",
    "    ax[0].plot(val_loss, c=\"r\", label=\"Val\")\n",
    "    ax[0].set_ylim([0, 1])\n",
    "    ax[0].title.set_text(\"Loss\")\n",
    "    \n",
    "    ax[1].plot(train_acc, c=\"b\", label=\"Train\")\n",
    "    ax[1].plot(val_acc, c=\"r\", label=\"Val\")\n",
    "    ax[1].set_ylim([0, 1])\n",
    "    ax[1].title.set_text(\"Accuracy\")\n",
    "    \n",
    "    plt.title(\"Training performance\")\n",
    "    plt.legend()\n",
    "    plt.savefig(DATA / \"images/train_perf.png\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "Training end epoch 1:   Loss: 0.61182\tAcc: 0.76202\n",
      "Validation end epoch 1: Loss: 0.56154\tAcc: 0.75024\n",
      "Epoch 2/10\n",
      "Training end epoch 2:   Loss: 0.54042\tAcc: 0.76200\n",
      "Validation end epoch 2: Loss: 0.54933\tAcc: 0.75024\n",
      "Epoch 3/10\n",
      "Training end epoch 3:   Loss: 0.53383\tAcc: 0.76202\n",
      "Validation end epoch 3: Loss: 0.54491\tAcc: 0.75024\n",
      "Epoch 4/10\n",
      "Training end epoch 4:   Loss: 0.52953\tAcc: 0.76201\n",
      "Validation end epoch 4: Loss: 0.54023\tAcc: 0.75024\n",
      "Epoch 5/10\n",
      "Training end epoch 5:   Loss: 0.52576\tAcc: 0.76197\n",
      "Validation end epoch 5: Loss: 0.53733\tAcc: 0.75024\n",
      "Epoch 6/10\n",
      "Training end epoch 6:   Loss: 0.51987\tAcc: 0.76202\n",
      "Validation end epoch 6: Loss: 0.53162\tAcc: 0.75024\n",
      "Epoch 7/10\n",
      "Training end epoch 7:   Loss: 0.51298\tAcc: 0.76276\n",
      "Validation end epoch 7: Loss: 0.52558\tAcc: 0.75244\n",
      "Epoch 8/10\n",
      "Training end epoch 8:   Loss: 0.50706\tAcc: 0.76592\n",
      "Validation end epoch 8: Loss: 0.51719\tAcc: 0.75562\n",
      "Epoch 9/10\n",
      "Training end epoch 9:   Loss: 0.50031\tAcc: 0.76973\n",
      "Validation end epoch 9: Loss: 0.51343\tAcc: 0.75952\n",
      "Epoch 10/10\n",
      "Training end epoch 10:   Loss: 0.49433\tAcc: 0.77101\n",
      "Validation end epoch 10: Loss: 0.50964\tAcc: 0.76099\n",
      "\n",
      "Testing: Loss: 0.50686\tAcc: 0.76307\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data\\\\images\\\\train_perf.png'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [11]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(DATA \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfinal_model.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m      2\u001b[0m     torch\u001b[38;5;241m.\u001b[39mmanual_seed(\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;66;03m# torch.save(model.state_dict(), DATA / \"final_model.pth\")\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m      6\u001b[0m     model \u001b[38;5;241m=\u001b[39m DTINetwork()\n",
      "Input \u001b[1;32mIn [10]\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(num_epochs)\u001b[0m\n\u001b[0;32m     88\u001b[0m ax[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mtitle\u001b[38;5;241m.\u001b[39mset_text(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     90\u001b[0m plt\u001b[38;5;241m.\u001b[39mtitle(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining performance\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 91\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msavefig\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDATA\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mimages/train_perf.png\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     92\u001b[0m plt\u001b[38;5;241m.\u001b[39mlegend()\n\u001b[0;32m     93\u001b[0m plt\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\teachopencadd\\lib\\site-packages\\matplotlib\\pyplot.py:954\u001b[0m, in \u001b[0;36msavefig\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    951\u001b[0m \u001b[38;5;129m@_copy_docstring_and_deprecators\u001b[39m(Figure\u001b[38;5;241m.\u001b[39msavefig)\n\u001b[0;32m    952\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msavefig\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    953\u001b[0m     fig \u001b[38;5;241m=\u001b[39m gcf()\n\u001b[1;32m--> 954\u001b[0m     res \u001b[38;5;241m=\u001b[39m fig\u001b[38;5;241m.\u001b[39msavefig(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    955\u001b[0m     fig\u001b[38;5;241m.\u001b[39mcanvas\u001b[38;5;241m.\u001b[39mdraw_idle()  \u001b[38;5;66;03m# Need this if 'transparent=True', to reset colors.\u001b[39;00m\n\u001b[0;32m    956\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\teachopencadd\\lib\\site-packages\\matplotlib\\figure.py:3274\u001b[0m, in \u001b[0;36mFigure.savefig\u001b[1;34m(self, fname, transparent, **kwargs)\u001b[0m\n\u001b[0;32m   3270\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m ax \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes:\n\u001b[0;32m   3271\u001b[0m         stack\u001b[38;5;241m.\u001b[39menter_context(\n\u001b[0;32m   3272\u001b[0m             ax\u001b[38;5;241m.\u001b[39mpatch\u001b[38;5;241m.\u001b[39m_cm_set(facecolor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnone\u001b[39m\u001b[38;5;124m'\u001b[39m, edgecolor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnone\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m-> 3274\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcanvas\u001b[38;5;241m.\u001b[39mprint_figure(fname, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\teachopencadd\\lib\\site-packages\\matplotlib\\backend_bases.py:2338\u001b[0m, in \u001b[0;36mFigureCanvasBase.print_figure\u001b[1;34m(self, filename, dpi, facecolor, edgecolor, orientation, format, bbox_inches, pad_inches, bbox_extra_artists, backend, **kwargs)\u001b[0m\n\u001b[0;32m   2334\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   2335\u001b[0m     \u001b[38;5;66;03m# _get_renderer may change the figure dpi (as vector formats\u001b[39;00m\n\u001b[0;32m   2336\u001b[0m     \u001b[38;5;66;03m# force the figure dpi to 72), so we need to set it again here.\u001b[39;00m\n\u001b[0;32m   2337\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m cbook\u001b[38;5;241m.\u001b[39m_setattr_cm(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfigure, dpi\u001b[38;5;241m=\u001b[39mdpi):\n\u001b[1;32m-> 2338\u001b[0m         result \u001b[38;5;241m=\u001b[39m print_method(\n\u001b[0;32m   2339\u001b[0m             filename,\n\u001b[0;32m   2340\u001b[0m             facecolor\u001b[38;5;241m=\u001b[39mfacecolor,\n\u001b[0;32m   2341\u001b[0m             edgecolor\u001b[38;5;241m=\u001b[39medgecolor,\n\u001b[0;32m   2342\u001b[0m             orientation\u001b[38;5;241m=\u001b[39morientation,\n\u001b[0;32m   2343\u001b[0m             bbox_inches_restore\u001b[38;5;241m=\u001b[39m_bbox_inches_restore,\n\u001b[0;32m   2344\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   2345\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m   2346\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m bbox_inches \u001b[38;5;129;01mand\u001b[39;00m restore_bbox:\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\teachopencadd\\lib\\site-packages\\matplotlib\\backend_bases.py:2204\u001b[0m, in \u001b[0;36mFigureCanvasBase._switch_canvas_and_return_print_method.<locals>.<lambda>\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   2200\u001b[0m     optional_kws \u001b[38;5;241m=\u001b[39m {  \u001b[38;5;66;03m# Passed by print_figure for other renderers.\u001b[39;00m\n\u001b[0;32m   2201\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdpi\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfacecolor\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124medgecolor\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124morientation\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   2202\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbbox_inches_restore\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[0;32m   2203\u001b[0m     skip \u001b[38;5;241m=\u001b[39m optional_kws \u001b[38;5;241m-\u001b[39m {\u001b[38;5;241m*\u001b[39minspect\u001b[38;5;241m.\u001b[39msignature(meth)\u001b[38;5;241m.\u001b[39mparameters}\n\u001b[1;32m-> 2204\u001b[0m     print_method \u001b[38;5;241m=\u001b[39m functools\u001b[38;5;241m.\u001b[39mwraps(meth)(\u001b[38;5;28;01mlambda\u001b[39;00m \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: meth(\n\u001b[0;32m   2205\u001b[0m         \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m{k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m skip}))\n\u001b[0;32m   2206\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# Let third-parties do as they see fit.\u001b[39;00m\n\u001b[0;32m   2207\u001b[0m     print_method \u001b[38;5;241m=\u001b[39m meth\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\teachopencadd\\lib\\site-packages\\matplotlib\\_api\\deprecation.py:410\u001b[0m, in \u001b[0;36mdelete_parameter.<locals>.wrapper\u001b[1;34m(*inner_args, **inner_kwargs)\u001b[0m\n\u001b[0;32m    400\u001b[0m     deprecation_addendum \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    401\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIf any parameter follows \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m, they should be passed as \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    402\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeyword, not positionally.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    403\u001b[0m     warn_deprecated(\n\u001b[0;32m    404\u001b[0m         since,\n\u001b[0;32m    405\u001b[0m         name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mrepr\u001b[39m(name),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    408\u001b[0m                  \u001b[38;5;28;01melse\u001b[39;00m deprecation_addendum,\n\u001b[0;32m    409\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m--> 410\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39minner_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minner_kwargs)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\teachopencadd\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:517\u001b[0m, in \u001b[0;36mFigureCanvasAgg.print_png\u001b[1;34m(self, filename_or_obj, metadata, pil_kwargs, *args)\u001b[0m\n\u001b[0;32m    468\u001b[0m \u001b[38;5;129m@_api\u001b[39m\u001b[38;5;241m.\u001b[39mdelete_parameter(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m3.5\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124margs\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    469\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprint_png\u001b[39m(\u001b[38;5;28mself\u001b[39m, filename_or_obj, \u001b[38;5;241m*\u001b[39margs,\n\u001b[0;32m    470\u001b[0m               metadata\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, pil_kwargs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    471\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    472\u001b[0m \u001b[38;5;124;03m    Write the figure to a PNG file.\u001b[39;00m\n\u001b[0;32m    473\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    515\u001b[0m \u001b[38;5;124;03m        *metadata*, including the default 'Software' key.\u001b[39;00m\n\u001b[0;32m    516\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 517\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_print_pil\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename_or_obj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpng\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpil_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\teachopencadd\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:464\u001b[0m, in \u001b[0;36mFigureCanvasAgg._print_pil\u001b[1;34m(self, filename_or_obj, fmt, pil_kwargs, metadata)\u001b[0m\n\u001b[0;32m    459\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    460\u001b[0m \u001b[38;5;124;03mDraw the canvas, then save it using `.image.imsave` (to which\u001b[39;00m\n\u001b[0;32m    461\u001b[0m \u001b[38;5;124;03m*pil_kwargs* and *metadata* are forwarded).\u001b[39;00m\n\u001b[0;32m    462\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    463\u001b[0m FigureCanvasAgg\u001b[38;5;241m.\u001b[39mdraw(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m--> 464\u001b[0m \u001b[43mmpl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimsave\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    465\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilename_or_obj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuffer_rgba\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfmt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morigin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mupper\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    466\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdpi\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfigure\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdpi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpil_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpil_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\teachopencadd\\lib\\site-packages\\matplotlib\\image.py:1664\u001b[0m, in \u001b[0;36mimsave\u001b[1;34m(fname, arr, vmin, vmax, cmap, format, origin, dpi, metadata, pil_kwargs)\u001b[0m\n\u001b[0;32m   1662\u001b[0m pil_kwargs\u001b[38;5;241m.\u001b[39msetdefault(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mformat\u001b[39m)\n\u001b[0;32m   1663\u001b[0m pil_kwargs\u001b[38;5;241m.\u001b[39msetdefault(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdpi\u001b[39m\u001b[38;5;124m\"\u001b[39m, (dpi, dpi))\n\u001b[1;32m-> 1664\u001b[0m image\u001b[38;5;241m.\u001b[39msave(fname, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpil_kwargs)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\teachopencadd\\lib\\site-packages\\PIL\\Image.py:2317\u001b[0m, in \u001b[0;36mImage.save\u001b[1;34m(self, fp, format, **params)\u001b[0m\n\u001b[0;32m   2315\u001b[0m         fp \u001b[38;5;241m=\u001b[39m builtins\u001b[38;5;241m.\u001b[39mopen(filename, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr+b\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   2316\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2317\u001b[0m         fp \u001b[38;5;241m=\u001b[39m \u001b[43mbuiltins\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mw+b\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2319\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   2320\u001b[0m     save_handler(\u001b[38;5;28mself\u001b[39m, fp, filename)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data\\\\images\\\\train_perf.png'"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 800x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if not os.path.exists(DATA / \"final_model.pth\"):\n",
    "    torch.manual_seed(42)\n",
    "    model = train(10)\n",
    "    # torch.save(model.state_dict(), DATA / \"final_model.pth\")\n",
    "else:\n",
    "    model = DTINetwork()\n",
    "    model.load_state_dict(torch.load(DATA / \"final_model.pth\"))\n",
    "    model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![TrainGraph](./images/train_perf.png)\n",
    "\n",
    "*Figure 3:*\n",
    "TDB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion\n",
    "\n",
    "Wrap up the talktorial's content here and discuss pros/cons and open questions/challenges."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz\n",
    "\n",
    "With this quiz you can test if you unterstand the important lessons of this talktorial.\n",
    "\n",
    "1. Why do we use structual data instead of amino acid sequences and SMILES strings?\n",
    "2. How do we convert proteins into graphs? What are the essential parts of proteins we use for that?\n",
    "3. Difficult: Why do we need to implement our own class to represent data points?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "<b>Useful checks at the end</b>: \n",
    "    \n",
    "<ul>\n",
    "<li>Clear output and rerun your complete notebook. Does it finish without errors?</li>\n",
    "<li>Check if your talktorial's runtime is as excepted. If not, try to find out which step(s) take unexpectedly long.</li>\n",
    "<li>Flag code cells with <code># NBVAL_CHECK_OUTPUT</code> that have deterministic output and should be tested within our Continuous Integration (CI) framework.</li>\n",
    "</ul>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
